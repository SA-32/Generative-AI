{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0fe63f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6fdaa169",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride):\n",
    "        super().__init__()\n",
    "        self.layer = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size = 4, stride = stride, padding = 1, padding_mode = 'reflect'),\n",
    "            nn.InstanceNorm2d(out_channels),\n",
    "            nn.LeakyReLU(0.2, inplace = True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f26f06a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, in_channels = 3, out_channels = [64, 128, 256, 512]):\n",
    "        super().__init__()\n",
    "        self.initial = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels[0], kernel_size = 4, stride = 2, padding = 1),\n",
    "            nn.LeakyReLU(0.2, inplace = True)\n",
    "        )\n",
    "        layers = [Block(out_channels[i], out_channels[i+1], stride = 1 if i == len(out_channels) - 2 else 2) for i in range(len(out_channels) - 1)]\n",
    "        layers.append(nn.Conv2d(out_channels[-1], 1, kernel_size = 4, stride = 1, padding = 1, padding_mode = 'reflect'))\n",
    "        self.backbone = nn.Sequential(\n",
    "            *layers\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.initial(x)\n",
    "        x = self.backbone(x)\n",
    "\n",
    "        return torch.sigmoid(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "77611a70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 30, 30])\n"
     ]
    }
   ],
   "source": [
    "D_model = Discriminator()\n",
    "test = torch.rand(1,3,256,256)\n",
    "print(D_model(test).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f0370547",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, inp, out, k, s, p, down = True, act = True):\n",
    "        super().__init__()\n",
    "        self.layer = nn.Sequential(\n",
    "            nn.Conv2d(inp, out, kernel_size = k, stride = s, padding = p, padding_mode = 'reflect') if down else nn.ConvTranspose2d(inp, out, kernel_size = k, stride = s, padding = p),\n",
    "            nn.InstanceNorm2d(out),\n",
    "            nn.ReLU(inplace = True) if act else nn.Identity()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5006ca46",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, features):\n",
    "        super().__init__()\n",
    "        self.layer = nn.Sequential(\n",
    "            ConvBlock(features, features, 3, 1, 1, True, True),\n",
    "            ConvBlock(features, features, 3, 1, 1, True, False)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.layer(x)\n",
    "        return x + y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "258bed51",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, img_channels = 3, num_features = 64, num_residuals = 9):\n",
    "        super().__init__()\n",
    "        self.initial = nn.Sequential(\n",
    "            nn.Conv2d(img_channels, num_features, kernel_size = 7, stride = 1, padding = 3, padding_mode = 'reflect'),\n",
    "            nn.InstanceNorm2d(num_features),\n",
    "            nn.ReLU(inplace = True)\n",
    "        )\n",
    "\n",
    "        self.down = nn.Sequential(\n",
    "            *[ConvBlock(num_features * (1 if i == 0 else 2), num_features * (2 if i == 0 else 4), 3, 2, 1) for i in range(2)]\n",
    "        )\n",
    "\n",
    "        self.res_blocks = nn.Sequential(\n",
    "            *[ResBlock(num_features * 4) for i in range(num_residuals)]            \n",
    "        )\n",
    "\n",
    "        self.up = nn.Sequential(\n",
    "            *[ConvBlock(num_features * 4, num_features * 2, 4, 2, 1, down = False), \n",
    "              ConvBlock(num_features * 2, num_features, 4, 2, 1, down = False)]\n",
    "        )\n",
    "\n",
    "        self.last = nn.Sequential(\n",
    "            nn.Conv2d(num_features, img_channels, kernel_size = 7, stride = 1, padding = 3, padding_mode = 'reflect')\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.initial(x)\n",
    "        x = self.down(x)\n",
    "        x = self.res_blocks(x)\n",
    "        x = self.up(x)\n",
    "        x = self.last(x)\n",
    "        return torch.tanh(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e25c7d72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "G_model = Generator()\n",
    "test = torch.rand(1,3,256,256)\n",
    "print(G_model(test).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f2b679",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.device.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7200d98e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model():\n",
    "    def __init__(self):\n",
    "        self.Gen_A = Generator().to(device)\n",
    "        self.Gen_B = Generator().to(device)\n",
    "        self.Disc_A = Discriminator().to(device)\n",
    "        self.Disc_B = Discriminator().to(device)\n",
    "\n",
    "        self.Gen_A_opt = torch.optim.Adam(self.Gen_A.parameters(), lr = 0.0001)\n",
    "        self.Gen_B_opt = torch.optim.Adam(self.Gen_B.parameters(), lr = 0.0001)\n",
    "        self.Disc_A_opt = torch.optim.Adam(self.Disc_A.parameters(), lr = 0.0001)\n",
    "        self.Disc_B_opt = torch.optim.Adam(self.Disc_B.parameters(), lr = 0.0001)\n",
    "\n",
    "        self.criterion = nn.BCELoss()\n",
    "        self.l1 = nn.L1Loss()\n",
    "\n",
    "    def train(self, real_black, real_gray):\n",
    "        # -------------------------\n",
    "        # Train Discriminator A\n",
    "        # -------------------------\n",
    "        fake_gray = self.Gen_A(real_black)\n",
    "        real_gray_pred = self.Disc_A(real_gray)\n",
    "        fake_gray_pred = self.Disc_A(fake_gray.detach())\n",
    "\n",
    "        real_gray_loss = self.criterion(real_gray_pred, torch.ones_like(real_gray_pred))\n",
    "        fake_gray_loss = self.criterion(fake_gray_pred, torch.zeros_like(fake_gray_pred))\n",
    "        disc_gray_loss = 0.5 * (real_gray_loss + fake_gray_loss)\n",
    "\n",
    "        self.Disc_A_opt.zero_grad()\n",
    "        disc_gray_loss.backward()\n",
    "        self.Disc_A_opt.step()\n",
    "\n",
    "        # -------------------------\n",
    "        # Train Discriminator B\n",
    "        # -------------------------\n",
    "        fake_black = self.Gen_B(real_gray)\n",
    "        real_black_pred = self.Disc_B(real_black)\n",
    "        fake_black_pred = self.Disc_B(fake_black.detach())\n",
    "\n",
    "        real_black_loss = self.criterion(real_black_pred, torch.ones_like(real_black_pred))\n",
    "        fake_black_loss = self.criterion(fake_black_pred, torch.zeros_like(fake_black_pred))\n",
    "        disc_black_loss = 0.5 * (real_black_loss + fake_black_loss)\n",
    "\n",
    "        self.Disc_B_opt.zero_grad()\n",
    "        disc_black_loss.backward()\n",
    "        self.Disc_B_opt.step()\n",
    "\n",
    "        # -------------------------\n",
    "        # Train Generator A (Black→Gray)\n",
    "        # -------------------------\n",
    "        gray_adversarial_loss = self.criterion(self.Disc_A(fake_gray), torch.ones_like(self.Disc_A(fake_gray)))\n",
    "        gray_cycle_loss = self.l1(self.Gen_B(fake_gray), real_black)\n",
    "        lambda_cycle = 10.0\n",
    "        gen_gray_loss = gray_adversarial_loss + lambda_cycle * gray_cycle_loss\n",
    "\n",
    "        self.Gen_A_opt.zero_grad()\n",
    "        gen_gray_loss.backward()\n",
    "        self.Gen_A_opt.step()\n",
    "\n",
    "        # -------------------------\n",
    "        # Train Generator B (Gray→Black)\n",
    "        # -------------------------\n",
    "        black_adversarial_loss = self.criterion(self.Disc_B(fake_black), torch.ones_like(self.Disc_B(fake_black)))\n",
    "        black_cycle_loss = self.l1(self.Gen_A(fake_black), real_gray)\n",
    "        gen_black_loss = black_adversarial_loss + lambda_cycle * black_cycle_loss\n",
    "\n",
    "        self.Gen_B_opt.zero_grad()\n",
    "        gen_black_loss.backward()\n",
    "        self.Gen_B_opt.step()\n",
    "\n",
    "        return {\n",
    "            \"disc_gray_loss\": disc_gray_loss.item(),\n",
    "            \"disc_black_loss\": disc_black_loss.item(),\n",
    "            \"gen_gray_loss\": gen_gray_loss.item(),\n",
    "            \"gen_black_loss\": gen_black_loss.item()\n",
    "        }"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
